{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae628058",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274bd184",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f6e14fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d409018c",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3b31806f",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_path = \"yuenning_model/qwen_gemma_multilabel_final_3.zip\" \n",
    "extract_path = \"yuenning_model/qwen_gemma_multilabel_final_3\" \n",
    "thresholds_path = \"yuenning_model/multilabel_thresholds.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45da774f",
   "metadata": {},
   "source": [
    "### 1. Unzip Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7f0a135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already extracted at: yuenning_model/qwen_gemma_multilabel_final_3\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(extract_path):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "    print(\"Model extracted to:\", extract_path)\n",
    "else:\n",
    "    print(\"Model already extracted at:\", extract_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a880b86",
   "metadata": {},
   "source": [
    "### 2. Load Model and Tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8ac3e5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen1.5-0.5B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(extract_path, num_labels=4)\n",
    "tokenizer = AutoTokenizer.from_pretrained(extract_path)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39c2db5",
   "metadata": {},
   "source": [
    "### 3. Load Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ee08faca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(thresholds_path, 'r') as f:\n",
    "    thresholds = json.load(f)\n",
    "\n",
    "label_cols = [\"is_ad\", \"is_relevant\", \"is_rant\", \"is_legit\"]\n",
    "with open(\"yuenning_model/multilabel_thresholds.json\", \"r\") as f:\n",
    "    thresholds_dict = json.load(f)\n",
    "thresholds_array = np.array([thresholds_dict[label] for label in label_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0adba2",
   "metadata": {},
   "source": [
    "### 4. Set Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9b8f981e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "print(\"Model loaded on\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6f8212",
   "metadata": {},
   "source": [
    "### 5. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "411c4b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_multilabel(texts, model, tokenizer, thresholds_array, batch_size=8, device=\"cuda\"):\n",
    "    all_probs = []\n",
    "    all_preds = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits  # shape: (batch_size, num_labels)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()  # shape: (batch_size, num_labels)\n",
    "            preds = (probs >= thresholds_array).astype(int)\n",
    "        \n",
    "        all_probs.extend(probs)\n",
    "        all_preds.extend(preds)\n",
    "    \n",
    "    return np.array(all_preds), np.array(all_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b19ee128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels:\n",
      " [[0 1 1 1]\n",
      " [0 1 1 1]]\n",
      "Probabilities:\n",
      " [[0.01301506 0.92230546 0.98700804 0.2625444 ]\n",
      " [0.01769127 0.7007936  0.94883215 0.97165   ]]\n",
      "   is_ad  is_relevant  is_rant  is_legit  is_ad_prob  is_relevant_prob  \\\n",
      "0      0            1        1         1    0.013015          0.922305   \n",
      "1      0            1        1         1    0.017691          0.700794   \n",
      "\n",
      "   is_rant_prob  is_legit_prob  \n",
      "0      0.987008       0.262544  \n",
      "1      0.948832       0.971650  \n"
     ]
    }
   ],
   "source": [
    "sample_texts = [\"This is an ad.\", \"The product is fantastic!\"]\n",
    "y_pred, y_probs = predict_multilabel(sample_texts, model, tokenizer, thresholds_array, device=device)\n",
    "print(\"Predicted labels:\\n\", y_pred)\n",
    "print(\"Probabilities:\\n\", y_probs)\n",
    "pred_df = pd.DataFrame(y_pred, columns=label_cols)\n",
    "probs_df = pd.DataFrame(y_probs, columns=[f\"{c}_prob\" for c in label_cols])\n",
    "results_df = pd.concat([pred_df, probs_df], axis=1)\n",
    "print(results_df)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
