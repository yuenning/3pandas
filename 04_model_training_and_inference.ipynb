{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9638c6d0",
   "metadata": {},
   "source": [
    "# **Problem Statement 1**  \n",
    "### **Filtering the Noise: ML for Trustworthy Location Reviews**  \n",
    "**Team 3Pandas** *(Tran Ha My, Diane Teo Min Xuan, Ng Yuen Ning)*  \n",
    "\n",
    "---\n",
    "\n",
    "## **Problem Statement**  \n",
    "Design and implement an **ML-based system** to evaluate the **quality** and **relevancy** of Google location reviews. The system should:  \n",
    "\n",
    "- **Gauge review quality:** Detect spam, advertisements, irrelevant content, and rants from users who have likely never visited the location.  \n",
    "- **Assess relevancy:** Determine whether the content of a review is genuinely related to the location being reviewed.  \n",
    "- **Enforce policies:** Automatically flag or filter out reviews that violate the following example policies:  \n",
    "  - No advertisements or promotional content.  \n",
    "  - No irrelevant content (e.g., reviews about unrelated topics).  \n",
    "  - No rants or complaints from users who have not visited the place (can be inferred from content, metadata, or other signals).  \n",
    "\n",
    "---\n",
    "\n",
    "## **Motivation & Impact**  \n",
    "- **For Users:** Increases trust in location-based reviews, leading to better decision-making.  \n",
    "- **For Businesses:** Ensures fair representation and reduces the impact of malicious or irrelevant reviews.  \n",
    "- **For Platforms:** Automates moderation, reduces manual workload, and enhances platform credibility.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Data Sources**  \n",
    "\n",
    "| **Data Sources**       | **Details** |\n",
    "|-------------------------|-------------|\n",
    "| **Public Datasets**    | - **Google Review Data:** Open datasets containing Google location reviews (e.g., [Google Local Reviews on Kaggle](https://www.kaggle.com/datasets/denizbilginn/google-maps-restaurant-reviews))<br>- **Google Local review data:** [UCSD Public Dataset](https://mcauleylab.ucsd.edu/public_datasets/gdrive/googlelocal/)<br>- **Alternative Sources:** Yelp, TripAdvisor, or other open review datasets for supplementary training. |\n",
    "| **Student-Crawled Data** | - Students are encouraged to crawl additional reviews from Google Maps (in compliance with Google's terms of service).<br>- **Example:** [Scraping Google Reviews (YouTube)](https://www.youtube.com/watch?v=LYMdZ7W9bWQ) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1a970a",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a575997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\ningy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ningy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# ! pip install transformers\n",
    "! pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a2762e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install tldextract\n",
    "import re\n",
    "import tldextract\n",
    "\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ! pip install textblob\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b426534",
   "metadata": {},
   "source": [
    "### 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e156759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfb37d5d",
   "metadata": {},
   "source": [
    "### 2. Pre-Process Datafames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6b8904",
   "metadata": {},
   "source": [
    "##### 2.1 Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f12b429b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_whitespace(text):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def clean_urls(text):\n",
    "    url_pattern = re.compile(r'https?://[^\\s]+')\n",
    "    urls = url_pattern.findall(text)\n",
    "    domains = [tldextract.extract(u).domain for u in urls]  # keep domains as tokens\n",
    "    text_cleaned = url_pattern.sub(' '.join(domains), text)\n",
    "    return text_cleaned\n",
    "\n",
    "def clean_text(text):\n",
    "    text = normalize_whitespace(text)\n",
    "    text = clean_urls(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c60b82",
   "metadata": {},
   "source": [
    "##### 2.2 Compute Basic Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7f4cc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_basic_signals(row):\n",
    "    url_count = len(re.findall(r'https?://\\S+', row['text']))\n",
    "    phone_count = len(re.findall(r'\\+?\\d[\\d\\s-]{7,}\\d', row['text']))\n",
    "    caps_ratio = sum(1 for c in row['text'] if c.isupper()) / max(len(row['text']), 1)\n",
    "    return url_count, phone_count, caps_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8e54ee",
   "metadata": {},
   "source": [
    "##### 2.3 Toxicity Signalling and Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f681c1d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m toxicity_pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-classification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munitary/toxic-bert\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_toxicity_scores_batch\u001b[39m(texts, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m):\n\u001b[0;32m      4\u001b[0m     scores \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\ningy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:998\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    993\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    994\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou cannot use both `pipeline(... torch_dtype=..., model_kwargs=\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:...})` as those\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    995\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m arguments might conflict, use only one.)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    996\u001b[0m         )\n\u001b[0;32m    997\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 998\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(torch_dtype, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[43mtorch\u001b[49m, torch_dtype):\n\u001b[0;32m    999\u001b[0m         torch_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(torch, torch_dtype)\n\u001b[0;32m   1000\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch_dtype\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "toxicity_pipeline = pipeline(\"text-classification\", model=\"unitary/toxic-bert\", truncation=True)\n",
    "\n",
    "def compute_toxicity_scores_batch(texts, batch_size=16):\n",
    "    scores = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        results = toxicity_pipeline(batch, truncation=True)\n",
    "        scores.extend([r['score'] for r in results])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf2489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_textblob_sentiment(text):\n",
    "    if pd.isna(text) or not isinstance(text, str) or text.strip() == \"\":\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    try:\n",
    "        analysis = TextBlob(text)\n",
    "        polarity = analysis.sentiment.polarity\n",
    "        subjectivity = analysis.sentiment.subjectivity\n",
    "        return polarity, subjectivity\n",
    "    except Exception:\n",
    "        return 0.0, 0.0\n",
    "\n",
    "sentiment_results = df[\"clean_text\"].apply(get_textblob_sentiment)\n",
    "df[\"sentiment_polarity\"], df[\"sentiment_subjectivity\"] = zip(*sentiment_results)\n",
    "\n",
    "# =====================\n",
    "# FILTER OUT EXTREME SENTIMENTS\n",
    "# =====================\n",
    "\n",
    "positive_threshold = 0.8\n",
    "negative_threshold = -0.8\n",
    "df[\"is_extreme_sentiment\"] = df[\"sentiment_polarity\"].apply(\n",
    "    lambda x: 1 if x >= positive_threshold or x <= negative_threshold else 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0298e4",
   "metadata": {},
   "source": [
    "##### Apply to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0173de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_reviews(df, timestamp_col=\"timestamp\"):\n",
    "    # Clean text\n",
    "    df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
    "\n",
    "    # Compute basic signals\n",
    "    signals = df.apply(lambda row: compute_basic_signals(row[\"clean_text\"], row.get(\"distance_m\", None)), axis=1)\n",
    "    df[\"url_count\"], df[\"phone_count\"], df[\"caps_ratio\"], df[\"distance_m\"] = zip(*signals)\n",
    "\n",
    "    # Compute toxicity\n",
    "    df[\"toxicity_score\"] = compute_toxicity_scores(df[\"clean_text\"].tolist())\n",
    "\n",
    "    # Ensure timestamp is datetime\n",
    "    df[timestamp_col] = pd.to_datetime(df[timestamp_col])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca9f773",
   "metadata": {},
   "source": [
    "### 3. Time-Based Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db795dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_time_based(df, timestamp_col=\"timestamp\"):\n",
    "    max_time = df[timestamp_col].max()\n",
    "    cut_train = max_time - pd.DateOffset(years=2)\n",
    "    cut_val = max_time - pd.DateOffset(months=6)\n",
    "    cut_test = max_time - pd.DateOffset(months=3)\n",
    "\n",
    "    train_df = df[(df[timestamp_col] >= cut_train) & (df[timestamp_col] < cut_val)]\n",
    "    val_df = df[(df[timestamp_col] >= cut_val) & (df[timestamp_col] < cut_test)]\n",
    "    test_df = df[df[timestamp_col] >= cut_test]\n",
    "\n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a89660",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
