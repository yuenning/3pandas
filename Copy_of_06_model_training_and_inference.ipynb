{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9638c6d0",
      "metadata": {
        "id": "9638c6d0"
      },
      "source": [
        "# **Problem Statement 1**  \n",
        "### **Filtering the Noise: ML for Trustworthy Location Reviews**  \n",
        "**Team 3Pandas** *(Tran Ha My, Diane Teo Min Xuan, Ng Yuen Ning)*  \n",
        "\n",
        "---\n",
        "\n",
        "## **Problem Statement**  \n",
        "Design and implement an **ML-based system** to evaluate the **quality** and **relevancy** of Google location reviews. The system should:  \n",
        "\n",
        "- **Gauge review quality:** Detect spam, advertisements, irrelevant content, and rants from users who have likely never visited the location.  \n",
        "- **Assess relevancy:** Determine whether the content of a review is genuinely related to the location being reviewed.  \n",
        "- **Enforce policies:** Automatically flag or filter out reviews that violate the following example policies:  \n",
        "  - No advertisements or promotional content.  \n",
        "  - No irrelevant content (e.g., reviews about unrelated topics).  \n",
        "  - No rants or complaints from users who have not visited the place (can be inferred from content, metadata, or other signals).  \n",
        "\n",
        "---\n",
        "\n",
        "## **Motivation & Impact**  \n",
        "- **For Users:** Increases trust in location-based reviews, leading to better decision-making.  \n",
        "- **For Businesses:** Ensures fair representation and reduces the impact of malicious or irrelevant reviews.  \n",
        "- **For Platforms:** Automates moderation, reduces manual workload, and enhances platform credibility.  \n",
        "\n",
        "---\n",
        "\n",
        "## **Data Sources**  \n",
        "\n",
        "| **Data Sources**       | **Details** |\n",
        "|-------------------------|-------------|\n",
        "| **Public Datasets**    | - **Google Review Data:** Open datasets containing Google location reviews (e.g., [Google Local Reviews on Kaggle](https://www.kaggle.com/datasets/denizbilginn/google-maps-restaurant-reviews))<br>- **Google Local review data:** [UCSD Public Dataset](https://mcauleylab.ucsd.edu/public_datasets/gdrive/googlelocal/)<br>- **Alternative Sources:** Yelp, TripAdvisor, or other open review datasets for supplementary training. |\n",
        "| **Student-Crawled Data** | - Students are encouraged to crawl additional reviews from Google Maps (in compliance with Google's terms of service).<br>- **Example:** [Scraping Google Reviews (YouTube)](https://www.youtube.com/watch?v=LYMdZ7W9bWQ) |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e1a970a",
      "metadata": {
        "id": "0e1a970a"
      },
      "source": [
        "### Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install iterative-stratification\n",
        "! pip install tldextract\n",
        "!pip install -q transformers accelerate datasets bitsandbytes peft trl torch tensorboard\n",
        "!pip install -U \"huggingface_hub[cli]\"\n",
        "!pip install -q huggingface_hub\n",
        "!pip install -q einops\n",
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "id": "TntPihBO05xl",
        "outputId": "09a77d93-4728-4759-d25c-c15637b83a72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "TntPihBO05xl",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting iterative-stratification\n",
            "  Downloading iterative_stratification-0.1.9-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from iterative-stratification) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from iterative-stratification) (1.16.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from iterative-stratification) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->iterative-stratification) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->iterative-stratification) (3.6.0)\n",
            "Downloading iterative_stratification-0.1.9-py3-none-any.whl (8.5 kB)\n",
            "Installing collected packages: iterative-stratification\n",
            "Successfully installed iterative-stratification-0.1.9\n",
            "Collecting tldextract\n",
            "  Downloading tldextract-5.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from tldextract) (3.10)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from tldextract) (2.32.4)\n",
            "Collecting requests-file>=1.4 (from tldextract)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.12/dist-packages (from tldextract) (3.19.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.1.0->tldextract) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.1.0->tldextract) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.1.0->tldextract) (2025.8.3)\n",
            "Downloading tldextract-5.3.0-py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Installing collected packages: requests-file, tldextract\n",
            "Successfully installed requests-file-2.1.0 tldextract-5.3.0\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.9/511.9 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface_hub[cli] in /usr/local/lib/python3.12/dist-packages (0.34.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[cli]) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[cli]) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[cli]) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[cli]) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[cli]) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[cli]) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[cli]) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[cli]) (1.1.8)\n",
            "Collecting InquirerPy==0.3.4 (from huggingface_hub[cli])\n",
            "  Downloading InquirerPy-0.3.4-py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting pfzy<0.4.0,>=0.3.1 (from InquirerPy==0.3.4->huggingface_hub[cli])\n",
            "  Downloading pfzy-0.3.4-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (3.0.51)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub[cli]) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub[cli]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub[cli]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub[cli]) (2025.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface_hub[cli]) (0.2.13)\n",
            "Downloading InquirerPy-0.3.4-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pfzy-0.3.4-py3-none-any.whl (8.5 kB)\n",
            "Installing collected packages: pfzy, InquirerPy\n",
            "Successfully installed InquirerPy-0.3.4 pfzy-0.3.4\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.47.0)\n",
            "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "id": "b1a2762e",
      "metadata": {
        "id": "b1a2762e"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "import os\n",
        "import json\n",
        "\n",
        "# ! pip install tldextract\n",
        "import re\n",
        "import tldextract\n",
        "\n",
        "from transformers import pipeline\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ! pip install tldextract\n",
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from torch.cuda import amp\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from transformers import pipeline\n",
        "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import IterableDataset, DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import torch.nn as nn\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import precision_recall_fscore_support, average_precision_score\n",
        "\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import gc\n",
        "import psutil\n",
        "from huggingface_hub import login\n",
        "\n",
        "from getpass import getpass\n",
        "import bitsandbytes as bnb\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "import shutil\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b426534",
      "metadata": {
        "id": "4b426534"
      },
      "source": [
        "### 1. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "ktcGgrRw0OWM",
        "outputId": "e1c4b0f4-a3a9-4f13-e430-bbd66a9f9ff5"
      },
      "id": "ktcGgrRw0OWM",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4fce568f-d19c-4890-ac0e-5a20c6c0d25f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4fce568f-d19c-4890-ac0e-5a20c6c0d25f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving all_reviews_with_labels_normalised.csv to all_reviews_with_labels_normalised.csv\n",
            "Saving synthetic_combined.csv to synthetic_combined.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_reviews = list(uploaded.keys())[0]\n",
        "synthetic_combined = list(uploaded.keys())[1]"
      ],
      "metadata": {
        "id": "JgSLDXqZ0nR6"
      },
      "id": "JgSLDXqZ0nR6",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "95d9dd7a",
      "metadata": {
        "id": "95d9dd7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "639cfa1b-4529-4551-fbe1-3ab653956f4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded all_reviews_with_labels_normalised.csv with 11667 rows\n",
            "review_text             0\n",
            "rating                  0\n",
            "has_photo               0\n",
            "author_name             0\n",
            "user_review_count       0\n",
            "business_name           0\n",
            "category                0\n",
            "source                  0\n",
            "review_id               0\n",
            "comprehensive_review    0\n",
            "is_ad                   0\n",
            "is_relevant             0\n",
            "is_rant                 0\n",
            "is_legit                0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# with open(\"config.yaml\", \"r\") as f:\n",
        "#     config = yaml.safe_load(f)\n",
        "\n",
        "# labeled_input_folder = config['labeled_input']\n",
        "# synthetic_folder = config['synthetic_folder']\n",
        "\n",
        "# full_df = pd.read_csv(f'{synthetic_folder}/all_reviews_with_labels_normalised.csv')\n",
        "# full_df.isnull().sum()\n",
        "\n",
        "full_df = pd.read_csv(all_reviews)\n",
        "full_df = full_df.dropna(subset=['rating']).reset_index(drop=True)\n",
        "\n",
        "print(f\"Loaded {all_reviews} with {len(full_df)} rows\")\n",
        "print(full_df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "synthetic_df = pd.read_csv(synthetic_combined)\n",
        "\n",
        "def s(col):\n",
        "    return synthetic_df[col].fillna(\"NA\").astype(str).str.strip()\n",
        "\n",
        "has_photo_str = np.where(synthetic_df[\"has_photo\"].fillna(False), \"yes\", \"no\")\n",
        "MAX_REVIEW_CHARS = 2000\n",
        "review_text_clean = s(\"review_text\").str.replace(r\"\\s+\", \" \", regex=True).str[:MAX_REVIEW_CHARS]\n",
        "\n",
        "synthetic_df[\"comprehensive_review\"] = (\n",
        "    \"[Business] \" + s(\"business_name\") +\n",
        "    \" | [Category] \" + s(\"category\") +\n",
        "    \" | [Rating] \" + s(\"rating\") +\n",
        "    \" | [Author] \" + s(\"author_name\") +\n",
        "    \" | [User Review Count] \" + s(\"user_review_count\") +\n",
        "    \" | [Has Photo] \" + pd.Series(has_photo_str, index=synthetic_df.index) +\n",
        "    \" | [Source] \" + s(\"source\") +\n",
        "    \" | [Review] \" + review_text_clean\n",
        ").str.replace(r\"\\s+\\|\\s+\\[Review\\]\\s+NA$\", \"\", regex=True)\n",
        "\n",
        "print(f\"Loaded {synthetic_combined} with {len(synthetic_df)} rows\")\n",
        "print(synthetic_df.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-5FU6p_0dv8",
        "outputId": "b5eeb93d-c585-4d5f-e03f-da590bebd6df"
      },
      "id": "l-5FU6p_0dv8",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded synthetic_combined.csv with 714 rows\n",
            "review_text             0\n",
            "rating                  0\n",
            "has_photo               0\n",
            "author_name             0\n",
            "user_review_count       0\n",
            "business_name           0\n",
            "category                0\n",
            "source                  0\n",
            "review_id               0\n",
            "is_ad                   0\n",
            "is_rant                 0\n",
            "is_legit                0\n",
            "is_relevant             0\n",
            "comprehensive_review    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "a08e3016",
      "metadata": {
        "id": "a08e3016",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "f3632155-707f-4f08-f227-a8bdbca75539"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                         review_text  rating  has_photo  \\\n",
              "0  Love the convenience of this neighborhood carw...     4.0      False   \n",
              "1  2 bathrooms (for a large 2 story building), 1 ...     2.0      False   \n",
              "2                 My favorite pizza shop hands down!     5.0      False   \n",
              "3  BOTCHED INSTRUMENT REPAIR IS COSTING US HUNDRE...     1.0      False   \n",
              "4                           Very unprofessional!!!!!     1.0      False   \n",
              "\n",
              "       author_name  user_review_count                         business_name  \\\n",
              "0     Doug Schmidt                1.0  Auto Spa Speedy Wash - Harvester, MO   \n",
              "1     Duf Duftopia                1.0                                 Kmart   \n",
              "2  Andrew Phillips                1.0                          Papa’s Pizza   \n",
              "3    Julie Heiland                1.0                       The Music Place   \n",
              "4    Alan Khasanov                1.0                   Park Motor Cars Inc   \n",
              "\n",
              "                                            category  source  review_id  \\\n",
              "0                                       ['Car wash']  google       1001   \n",
              "1  ['Discount store', 'Appliance store', 'Baby st...  google       1002   \n",
              "2  ['Pizza restaurant', 'Chicken wings restaurant...  google       1003   \n",
              "3                       ['Musical instrument store']  google       1004   \n",
              "4                                ['Used car dealer']  google       1005   \n",
              "\n",
              "                                comprehensive_review  is_ad  is_relevant  \\\n",
              "0  [Business] Auto Spa Speedy Wash - Harvester, M...  False         True   \n",
              "1  [Business] Kmart | [Category] ['Discount store...   True         True   \n",
              "2  [Business] Papa’s Pizza | [Category] ['Pizza r...  False         True   \n",
              "3  [Business] The Music Place | [Category] ['Musi...  False         True   \n",
              "4  [Business] Park Motor Cars Inc | [Category] ['...  False         True   \n",
              "\n",
              "   is_rant  is_legit  \n",
              "0    False      True  \n",
              "1     True     False  \n",
              "2    False      True  \n",
              "3     True     False  \n",
              "4     True     False  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-87063faf-c351-41af-a066-ccf0c0ab2e0c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_text</th>\n",
              "      <th>rating</th>\n",
              "      <th>has_photo</th>\n",
              "      <th>author_name</th>\n",
              "      <th>user_review_count</th>\n",
              "      <th>business_name</th>\n",
              "      <th>category</th>\n",
              "      <th>source</th>\n",
              "      <th>review_id</th>\n",
              "      <th>comprehensive_review</th>\n",
              "      <th>is_ad</th>\n",
              "      <th>is_relevant</th>\n",
              "      <th>is_rant</th>\n",
              "      <th>is_legit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Love the convenience of this neighborhood carw...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>False</td>\n",
              "      <td>Doug Schmidt</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Auto Spa Speedy Wash - Harvester, MO</td>\n",
              "      <td>['Car wash']</td>\n",
              "      <td>google</td>\n",
              "      <td>1001</td>\n",
              "      <td>[Business] Auto Spa Speedy Wash - Harvester, M...</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2 bathrooms (for a large 2 story building), 1 ...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>False</td>\n",
              "      <td>Duf Duftopia</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Kmart</td>\n",
              "      <td>['Discount store', 'Appliance store', 'Baby st...</td>\n",
              "      <td>google</td>\n",
              "      <td>1002</td>\n",
              "      <td>[Business] Kmart | [Category] ['Discount store...</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>My favorite pizza shop hands down!</td>\n",
              "      <td>5.0</td>\n",
              "      <td>False</td>\n",
              "      <td>Andrew Phillips</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Papa’s Pizza</td>\n",
              "      <td>['Pizza restaurant', 'Chicken wings restaurant...</td>\n",
              "      <td>google</td>\n",
              "      <td>1003</td>\n",
              "      <td>[Business] Papa’s Pizza | [Category] ['Pizza r...</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>BOTCHED INSTRUMENT REPAIR IS COSTING US HUNDRE...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>False</td>\n",
              "      <td>Julie Heiland</td>\n",
              "      <td>1.0</td>\n",
              "      <td>The Music Place</td>\n",
              "      <td>['Musical instrument store']</td>\n",
              "      <td>google</td>\n",
              "      <td>1004</td>\n",
              "      <td>[Business] The Music Place | [Category] ['Musi...</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Very unprofessional!!!!!</td>\n",
              "      <td>1.0</td>\n",
              "      <td>False</td>\n",
              "      <td>Alan Khasanov</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Park Motor Cars Inc</td>\n",
              "      <td>['Used car dealer']</td>\n",
              "      <td>google</td>\n",
              "      <td>1005</td>\n",
              "      <td>[Business] Park Motor Cars Inc | [Category] ['...</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-87063faf-c351-41af-a066-ccf0c0ab2e0c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-87063faf-c351-41af-a066-ccf0c0ab2e0c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-87063faf-c351-41af-a066-ccf0c0ab2e0c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-cbdf0a37-7708-46b7-b2e7-a6061549c9af\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cbdf0a37-7708-46b7-b2e7-a6061549c9af')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-cbdf0a37-7708-46b7-b2e7-a6061549c9af button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "to_clean_df",
              "summary": "{\n  \"name\": \"to_clean_df\",\n  \"rows\": 11667,\n  \"fields\": [\n    {\n      \"column\": \"review_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11194,\n        \"samples\": [\n          \"Nice place to go!  One of few places with a merry go round anymore.  Great for anything outside!\",\n          \"Great wings and fries\",\n          \"quick, home-style food with a small and simple menu. Milk shakes are very good\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rating\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.21894107157901,\n        \"min\": 1.0,\n        \"max\": 5.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2.0,\n          3.0,\n          5.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"has_photo\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true,\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"author_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11401,\n        \"samples\": [\n          \"Richard Jensen\",\n          \"Kelley Byrd\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"user_review_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 120.44379856716289,\n        \"min\": 0.0,\n        \"max\": 3062.0,\n        \"num_unique_values\": 448,\n        \"samples\": [\n          391.0,\n          120.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"business_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6124,\n        \"samples\": [\n          \"Nellis Main Exchange\",\n          \"HoDo Restaurant & Lounge\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3383,\n        \"samples\": [\n          \"['Banquet hall', 'Wedding venue']\",\n          \"['Hardware store', 'Grill store', 'Home improvement store', 'Lawn mower store', 'Paint store', 'Tool store']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"singapore\",\n          \"google\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"review_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5790,\n        \"min\": 1001,\n        \"max\": 20614,\n        \"num_unique_values\": 11667,\n        \"samples\": [\n          19717,\n          7835\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comprehensive_review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11659,\n        \"samples\": [\n          \"[Business] Regal UA Kaufman Astoria & RPX | [Category] ['Movie theater'] | [Rating] 3.0 | [Author] Osbaldo Minchala | [User Review Count] 1.0 | [Has Photo] no | [Source] google | [Review] It was okay. The movie i watched was the only good thing I liked about this place.\",\n          \"[Business] Walmart Supercenter | [Category] ['Department store', 'Clothing store', 'Craft store', 'Discount store', 'Electronics store', 'Grocery store', 'Home goods store', 'Sporting goods store', 'Supermarket', 'Toy store'] | [Rating] 1.0 | [Author] Courtney Widick | [User Review Count] 1.0 | [Has Photo] no | [Source] google | [Review] I do not understand why at 7pm on a Tuesday afternoon these only 2 registers open. This is ridiculous waiting in these long lines for a few items when there could be more then 2 registers going right now.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"is_ad\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true,\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"is_relevant\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          false,\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"is_rant\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true,\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"is_legit\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          false,\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "to_clean_df = full_df.dropna(subset=['review_text', 'is_ad', 'is_relevant', 'is_rant', 'is_legit'])\n",
        "\n",
        "to_clean_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfb37d5d",
      "metadata": {
        "id": "bfb37d5d"
      },
      "source": [
        "### 2. Pre-Process Datafames"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a6b8904",
      "metadata": {
        "id": "7a6b8904"
      },
      "source": [
        "##### 2.1 Cleaning Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "f12b429b",
      "metadata": {
        "id": "f12b429b"
      },
      "outputs": [],
      "source": [
        "def normalize_whitespace(text):\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = str(text)\n",
        "    text = normalize_whitespace(text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1c60b82",
      "metadata": {
        "id": "f1c60b82"
      },
      "source": [
        "##### 2.2 Compute Basic Signals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "e7f4cc3e",
      "metadata": {
        "id": "e7f4cc3e"
      },
      "outputs": [],
      "source": [
        "def compute_basic_signals(text):\n",
        "    url_count = len(re.findall(r'https?://\\S+', text))\n",
        "    phone_count = len(re.findall(r'\\+?\\d[\\d\\s-]{7,}\\d', text))\n",
        "    caps_ratio = sum(1 for c in text if c.isupper()) / max(len(text), 1)\n",
        "    return url_count, phone_count, caps_ratio"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c8e54ee",
      "metadata": {
        "id": "9c8e54ee"
      },
      "source": [
        "##### 2.3 Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "2bf2489e",
      "metadata": {
        "id": "2bf2489e"
      },
      "outputs": [],
      "source": [
        "def add_textblob_sentiment(df, text_col=\"review_text\", positive_threshold=0.9, negative_threshold=-0.9):\n",
        "    def get_sentiment(text):\n",
        "        if pd.isna(text) or not isinstance(text, str) or text.strip() == \"\":\n",
        "            return 0.0, 0.0\n",
        "        try:\n",
        "            analysis = TextBlob(text)\n",
        "            return analysis.sentiment.polarity, analysis.sentiment.subjectivity\n",
        "        except Exception:\n",
        "            return 0.0, 0.0\n",
        "\n",
        "    sentiment_results = df[text_col].apply(get_sentiment)\n",
        "    df[\"sentiment_polarity\"], df[\"sentiment_subjectivity\"] = zip(*sentiment_results)\n",
        "\n",
        "    df[\"is_extreme_sentiment\"] = df[\"sentiment_polarity\"].apply(\n",
        "        lambda x: 1 if x >= positive_threshold or x <= negative_threshold else 0\n",
        "    )\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b0298e4",
      "metadata": {
        "id": "8b0298e4"
      },
      "source": [
        "##### Apply to Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "7e0173de",
      "metadata": {
        "id": "7e0173de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e43dcb57-165a-44e6-bdfb-17ee8a6ae336"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                         review_text  rating  has_photo  \\\n",
            "0  Love the convenience of this neighborhood carw...     4.0      False   \n",
            "1  2 bathrooms (for a large 2 story building), 1 ...     2.0      False   \n",
            "2                 My favorite pizza shop hands down!     5.0      False   \n",
            "3  BOTCHED INSTRUMENT REPAIR IS COSTING US HUNDRE...     1.0      False   \n",
            "4                           Very unprofessional!!!!!     1.0      False   \n",
            "\n",
            "       author_name  user_review_count                         business_name  \\\n",
            "0     Doug Schmidt                1.0  Auto Spa Speedy Wash - Harvester, MO   \n",
            "1     Duf Duftopia                1.0                                 Kmart   \n",
            "2  Andrew Phillips                1.0                          Papa’s Pizza   \n",
            "3    Julie Heiland                1.0                       The Music Place   \n",
            "4    Alan Khasanov                1.0                   Park Motor Cars Inc   \n",
            "\n",
            "                                            category  source  review_id  \\\n",
            "0                                       ['Car wash']  google       1001   \n",
            "1  ['Discount store', 'Appliance store', 'Baby st...  google       1002   \n",
            "2  ['Pizza restaurant', 'Chicken wings restaurant...  google       1003   \n",
            "3                       ['Musical instrument store']  google       1004   \n",
            "4                                ['Used car dealer']  google       1005   \n",
            "\n",
            "                                comprehensive_review  is_ad  is_relevant  \\\n",
            "0  [Business] Auto Spa Speedy Wash - Harvester, M...  False         True   \n",
            "1  [Business] Kmart | [Category] ['Discount store...   True         True   \n",
            "2  [Business] Papa’s Pizza | [Category] ['Pizza r...  False         True   \n",
            "3  [Business] The Music Place | [Category] ['Musi...  False         True   \n",
            "4  [Business] Park Motor Cars Inc | [Category] ['...  False         True   \n",
            "\n",
            "   is_rant  is_legit                                         clean_text  \\\n",
            "0    False      True  Love the convenience of this neighborhood carw...   \n",
            "1     True     False  2 bathrooms (for a large 2 story building), 1 ...   \n",
            "2    False      True                 My favorite pizza shop hands down!   \n",
            "3     True     False  BOTCHED INSTRUMENT REPAIR IS COSTING US HUNDRE...   \n",
            "4     True     False                           Very unprofessional!!!!!   \n",
            "\n",
            "   url_count  phone_count  caps_ratio  \n",
            "0          0            0    0.020000  \n",
            "1          0            0    0.016949  \n",
            "2          0            0    0.029412  \n",
            "3          0            0    0.042589  \n",
            "4          0            0    0.041667  \n",
            "                                         review_text  rating  has_photo  \\\n",
            "0  Had a fantastic meal at The Gourmet Place! The...       5       True   \n",
            "1  Great experience at FitLife Gym! The equipment...       4      False   \n",
            "2  Staying at Oceanview Hotel was a dream! The vi...       5       True   \n",
            "3  The Family Clinic is amazing! They really take...       4      False   \n",
            "4  I love shopping at Trendy Mall! They have ever...       5       True   \n",
            "\n",
            "      author_name  user_review_count      business_name       category  \\\n",
            "0   Sophia Turner                 23  The Gourmet Place     restaurant   \n",
            "1   Michael Brown                 15        FitLife Gym            gym   \n",
            "2     Emily Davis                 30    Oceanview Hotel          hotel   \n",
            "3     Liam Wilson                 10      Family Clinic         clinic   \n",
            "4  Olivia Johnson                 28        Trendy Mall  shopping mall   \n",
            "\n",
            "   source  review_id  is_ad  is_rant  is_legit  is_relevant  \\\n",
            "0  Google      20615   True    False     False         True   \n",
            "1    Yelp      20616   True    False     False         True   \n",
            "2  Google      20617   True    False     False         True   \n",
            "3    Yelp      20618   True    False     False         True   \n",
            "4  Google      20619   True    False     False         True   \n",
            "\n",
            "                                comprehensive_review  \\\n",
            "0  [Business] The Gourmet Place | [Category] rest...   \n",
            "1  [Business] FitLife Gym | [Category] gym | [Rat...   \n",
            "2  [Business] Oceanview Hotel | [Category] hotel ...   \n",
            "3  [Business] Family Clinic | [Category] clinic |...   \n",
            "4  [Business] Trendy Mall | [Category] shopping m...   \n",
            "\n",
            "                                          clean_text  url_count  phone_count  \\\n",
            "0  Had a fantastic meal at The Gourmet Place! The...          1            0   \n",
            "1  Great experience at FitLife Gym! The equipment...          0            1   \n",
            "2  Staying at Oceanview Hotel was a dream! The vi...          1            0   \n",
            "3  The Family Clinic is amazing! They really take...          1            0   \n",
            "4  I love shopping at Trendy Mall! They have ever...          1            0   \n",
            "\n",
            "   caps_ratio  \n",
            "0    0.042424  \n",
            "1    0.041958  \n",
            "2    0.038168  \n",
            "3    0.034965  \n",
            "4    0.045045  \n"
          ]
        }
      ],
      "source": [
        "def preprocess_reviews(df):\n",
        "    df[\"clean_text\"] = df[\"review_text\"].apply(clean_text)\n",
        "    signals = df[\"clean_text\"].apply(compute_basic_signals)\n",
        "    df[\"url_count\"], df[\"phone_count\"], df[\"caps_ratio\"] = zip(*signals)\n",
        "    return df\n",
        "\n",
        "cleaned_df = preprocess_reviews(to_clean_df)\n",
        "print(cleaned_df.head())\n",
        "\n",
        "cleaned_synthetic_df = preprocess_reviews(synthetic_df)\n",
        "print(cleaned_synthetic_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "bab7dfc9",
      "metadata": {
        "id": "bab7dfc9"
      },
      "outputs": [],
      "source": [
        "# # Save as JSON\n",
        "# output_json_path = os.path.join(labeled_input_folder, \"cleaned_df.json\")\n",
        "# cleaned_df.to_json(output_json_path, orient=\"records\", lines=True, force_ascii=False)\n",
        "# print(f\"JSON file saved to: {output_json_path}\")\n",
        "\n",
        "# # Save as Parquet\n",
        "# output_parquet_path = os.path.join(labeled_input_folder, \"cleaned_df.parquet\")\n",
        "# cleaned_df.to_parquet(output_parquet_path, index=False)\n",
        "# print(f\"Parquet file saved to: {output_parquet_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ca9f773",
      "metadata": {
        "id": "6ca9f773"
      },
      "source": [
        "### 3. Train-Test Split with Multi-Label Stratification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "3235d024",
      "metadata": {
        "id": "3235d024",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7c2b5de-aade-4524-f895-ae67e21e507a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: (8181, 18) (includes synthetic data)\n",
            "Validation set size: (1866, 18)\n",
            "Test set size: (2334, 18)\n",
            "Synthetic data in validation set: False\n",
            "Synthetic data in test set: False\n"
          ]
        }
      ],
      "source": [
        "meta_cols = [\"clean_text\", \"url_count\",\"phone_count\",\"caps_ratio\",\"rating\",\"has_photo\",\"user_review_count\"]\n",
        "label_cols = [\"is_ad\",\"is_relevant\",\"is_rant\"]\n",
        "\n",
        "X = cleaned_df.drop(columns=label_cols)\n",
        "y = cleaned_df[label_cols].values\n",
        "\n",
        "mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "train_val_idx, test_idx = next(mskf.split(X, y))\n",
        "\n",
        "train_val_df = cleaned_df.iloc[train_val_idx].reset_index(drop=True)\n",
        "test_df = cleaned_df.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "y_train_val = y[train_val_idx]\n",
        "train_idx, val_idx = next(mskf.split(train_val_df.drop(columns=label_cols), y_train_val))\n",
        "\n",
        "train_df_original = train_val_df.iloc[train_idx].reset_index(drop=True)\n",
        "val_df = train_val_df.iloc[val_idx].reset_index(drop=True)\n",
        "\n",
        "train_df = pd.concat([train_df_original, cleaned_synthetic_df], ignore_index=True).reset_index(drop=True)\n",
        "\n",
        "print(f\"Training set size: {train_df.shape} (includes synthetic data)\")\n",
        "print(f\"Validation set size: {val_df.shape}\")\n",
        "print(f\"Test set size: {test_df.shape}\")\n",
        "\n",
        "print(f\"Synthetic data in validation set: {val_df['review_id'].isin(cleaned_synthetic_df['review_id']).any()}\")\n",
        "print(f\"Synthetic data in test set: {test_df['review_id'].isin(cleaned_synthetic_df['review_id']).any()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03ff6b75",
      "metadata": {
        "id": "03ff6b75"
      },
      "source": [
        "### 4. Tokenisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "2dc6aa88",
      "metadata": {
        "id": "2dc6aa88"
      },
      "outputs": [],
      "source": [
        "def simple_tokenize(text):\n",
        "    text = str(text).lower()\n",
        "    tokens = re.findall(r'\\b[a-z]+\\b', text)\n",
        "    return tokens\n",
        "\n",
        "train_df['tokens'] = train_df['clean_text'].apply(simple_tokenize)\n",
        "test_df['tokens'] = test_df['clean_text'].apply(simple_tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "c44b1329",
      "metadata": {
        "id": "c44b1329",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83b9f92d-a861-4e83-d3c5-a9732f5266ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8343, 19)\n",
            "(1907, 18)\n",
            "(2384, 19)\n"
          ]
        }
      ],
      "source": [
        "print(train_df.shape)\n",
        "print(val_df.shape)\n",
        "print(test_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66de5f2d",
      "metadata": {
        "id": "66de5f2d"
      },
      "source": [
        "### Yuen Ning's model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"Qwen/Qwen1.5-0.5B\"  # or \"google/gemma-2b\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "r36ZQQT1O0j_"
      },
      "id": "r36ZQQT1O0j_",
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3, torch_dtype=\"auto\")\n",
        "config = LoraConfig(r=8, lora_alpha=32, lora_dropout=0.1)\n",
        "model = get_peft_model(model, config)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = tokenizer.pad_token_id"
      ],
      "metadata": {
        "id": "B7nZHonMZxy5",
        "outputId": "501a4bf2-97f3-4f6a-ed53-396b3780d0c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "B7nZHonMZxy5",
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen1.5-0.5B and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./qwen_gemma_multilabel\",\n",
        "    # evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=3,\n",
        "    fp16=True,\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False # Add this line to ignore unused columns\n",
        ")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    probs = torch.sigmoid(torch.tensor(logits)).numpy()\n",
        "    preds = (probs > 0.5).astype(int)\n",
        "\n",
        "    precision = precision_score(labels, preds, average='macro', zero_division=0)\n",
        "    recall = recall_score(labels, preds, average='macro', zero_division=0)\n",
        "    f1 = f1_score(labels, preds, average='macro', zero_division=0)\n",
        "\n",
        "    return {\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1\n",
        "    }"
      ],
      "metadata": {
        "id": "JcxJvxOTZ9BM"
      },
      "id": "JcxJvxOTZ9BM",
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(batch):\n",
        "    return tokenizer(batch[\"clean_text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "def prepare_dataset(df):\n",
        "    # Apply filtering to the DataFrame directly\n",
        "    df = df.reset_index(drop=True)\n",
        "    return Dataset.from_pandas(df[[\"clean_text\"] + label_cols])\n",
        "\n",
        "# Filter train_df before creating the dataset\n",
        "train_df_filtered = train_df.drop(index=7026).reset_index(drop=True)\n",
        "\n",
        "train_dataset = prepare_dataset(train_df_filtered)\n",
        "val_dataset = prepare_dataset(val_df)\n",
        "\n",
        "train_dataset = train_dataset.map(preprocess, batched=True)\n",
        "val_dataset = val_dataset.map(preprocess, batched=True)\n",
        "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"is_ad\", \"is_relevant\", \"is_rant\"])\n",
        "val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"is_ad\", \"is_relevant\", \"is_rant\"])\n"
      ],
      "metadata": {
        "id": "koxpmBR_bLy-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "9bed4ee155924991ac3a8e0b171534f2",
            "a03a2e58cb704e24a8b7bb6f563a3e57",
            "9e6028ad6df34afbb334d1854317fa15",
            "c177bed50b44453ebf3075c0c8dca3e5",
            "c93d71abbbe14b73829dab5ef47fec32",
            "31f677824421490187a91716b7b13be1",
            "65a914f4a864483d9789f835716bd3d4",
            "47f0bd93dde44f7999a4a169cd817668",
            "2ad87e6e2af840c7afc6a79fb019effd",
            "89a6f136e1dc40eb9991d98eabae9bdd",
            "0743142c7e8f4bce8f8d811c2adc9566",
            "506a3e7320bd4da7a1d6dab730ef2f80",
            "b8d72c1dd5704d6eb345aa6738db184f",
            "9022492a03d84fb5ae3ae8de6a9c9bc5",
            "0d548295a9ed466bb73550974d02eca2",
            "8283989e3321459eac606fe4b09dda10",
            "0382a99d6fa7410980c568979c3b0a71",
            "75602bf9395046edb92ed6a80ceaf29a",
            "155b1addd236490193c475e900613594",
            "12e41a4a51794203b44c39a45dcce171",
            "37cf38389093453bb642d582138c20e0",
            "95f5a95ef2a34aea8a31d186731884a6"
          ]
        },
        "outputId": "360eea8c-9cee-49f0-cb13-773ac98e5e6f"
      },
      "id": "koxpmBR_bLy-",
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/8180 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9bed4ee155924991ac3a8e0b171534f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1866 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "506a3e7320bd4da7a1d6dab730ef2f80"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_collator(batch):\n",
        "    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n",
        "    attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n",
        "    labels = torch.stack([\n",
        "        torch.tensor([item[\"is_ad\"], item[\"is_relevant\"], item[\"is_rant\"]], dtype=torch.float)\n",
        "        for item in batch\n",
        "    ])\n",
        "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset, # Use the Hugging Face Dataset\n",
        "    eval_dataset=val_dataset,   # Use the Hugging Face Dataset\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "o0S0LuooaTn0",
        "outputId": "4548618b-71bd-4cac-955e-c705b9791d4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "o0S0LuooaTn0",
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3843213006.py:10: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "Bd6YlCjNfiWS",
        "outputId": "5eeb61c7-cd04-4524-8e99-5784155c7558"
      },
      "id": "Bd6YlCjNfiWS",
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1536' max='1536' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1536/1536 33:46, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.447100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.187000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.110400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1536, training_loss=0.24450827219213048, metrics={'train_runtime': 2028.9279, 'train_samples_per_second': 12.095, 'train_steps_per_second': 0.757, 'total_flos': 2.330922766565376e+16, 'train_loss': 0.24450827219213048, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./qwen_gemma_multilabel_final\")\n",
        "tokenizer.save_pretrained(\"./qwen_gemma_multilabel_final\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvYXo2HUooHJ",
        "outputId": "bb90b8cf-ad6d-4f99-bea1-17d2124bb0e2"
      },
      "id": "lvYXo2HUooHJ",
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./qwen_gemma_multilabel_final/tokenizer_config.json',\n",
              " './qwen_gemma_multilabel_final/special_tokens_map.json',\n",
              " './qwen_gemma_multilabel_final/chat_template.jinja',\n",
              " './qwen_gemma_multilabel_final/vocab.json',\n",
              " './qwen_gemma_multilabel_final/merges.txt',\n",
              " './qwen_gemma_multilabel_final/added_tokens.json',\n",
              " './qwen_gemma_multilabel_final/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = trainer.evaluate()\n",
        "print(eval_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "poHEa-qmow_u",
        "outputId": "fa20381c-6d18-4c91-e7a4-8ea470279e62"
      },
      "id": "poHEa-qmow_u",
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='468' max='234' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [234/234 02:41]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.13003966212272644, 'eval_precision': 0.7716463472962802, 'eval_recall': 0.6625803733996433, 'eval_f1': 0.7074768587565138, 'eval_runtime': 66.6749, 'eval_samples_per_second': 27.987, 'eval_steps_per_second': 3.51, 'epoch': 3.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.make_archive(\"qwen_gemma_multilabel_final\", 'zip', \"./qwen_gemma_multilabel_final\")\n",
        "files.download(\"qwen_gemma_multilabel_final.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "MpxiNr4FqViQ",
        "outputId": "513fc5d0-ad3f-4676-c8c1-a55bd8036612"
      },
      "id": "MpxiNr4FqViQ",
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_eee955fe-86df-417c-8908-f1cf243acbcb\", \"qwen_gemma_multilabel_final.zip\", 6860196)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### more graveyard"
      ],
      "metadata": {
        "id": "2ydi6f0q41QW"
      },
      "id": "2ydi6f0q41QW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b120940",
      "metadata": {
        "id": "0b120940"
      },
      "outputs": [],
      "source": [
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_len=128):\n",
        "        self.texts = df[\"clean_text\"].tolist()\n",
        "        self.meta = df[meta_cols].fillna(0).to_numpy(dtype=np.float32)\n",
        "        self.labels = df[label_cols].to_numpy(dtype=np.float32)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Ensure idx is a single int\n",
        "        if isinstance(idx, list):\n",
        "            idx = idx[0]\n",
        "\n",
        "        text = str(self.texts[idx])\n",
        "        meta = self.meta[idx]\n",
        "        labels = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
        "            \"meta\": torch.tensor(meta, dtype=torch.float),\n",
        "            \"labels\": torch.tensor(labels, dtype=torch.float)\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        \"input_ids\": torch.stack([b[\"input_ids\"] for b in batch]),\n",
        "        \"attention_mask\": torch.stack([b[\"attention_mask\"] for b in batch]),\n",
        "        \"meta\": torch.stack([b[\"meta\"] for b in batch]),\n",
        "        \"labels\": torch.stack([b[\"labels\"] for b in batch])\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9102cb43",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9102cb43",
        "outputId": "c1f4cac8-f4ed-494e-dd32-c00ac9e6d41e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen2-0.5B and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Qwen2ForSequenceClassification(\n",
              "  (model): Qwen2Model(\n",
              "    (embed_tokens): Embedding(151936, 896)\n",
              "    (layers): ModuleList(\n",
              "      (0-23): 24 x Qwen2DecoderLayer(\n",
              "        (self_attn): Qwen2Attention(\n",
              "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
              "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
              "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
              "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
              "        )\n",
              "        (mlp): Qwen2MLP(\n",
              "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
              "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
              "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "    (rotary_emb): Qwen2RotaryEmbedding()\n",
              "  )\n",
              "  (score): Linear(in_features=896, out_features=4, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "model_name = \"Qwen/Qwen2-0.5B\"  # lightweight Qwen\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B\", use_fast=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_cols))\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74b41c2d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74b41c2d",
        "outputId": "7fcb7cda-5fa7-4606-9d61-fddce4c5d82d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "batch_size = 8\n",
        "train_dataset = ReviewDataset(train_df, tokenizer)\n",
        "test_dataset = ReviewDataset(test_df, tokenizer)\n",
        "\n",
        "train_dataset = ReviewDataset(train_df, tokenizer, max_len=128)\n",
        "val_dataset = ReviewDataset(val_df, tokenizer, max_len=128)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    pin_memory=True,\n",
        "    num_workers=4\n",
        ")\n",
        "val_loader   = torch.utils.data.DataLoader(val_dataset, batch_size=32)\n",
        "test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "num_workers = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4c7cd3f",
      "metadata": {
        "id": "f4c7cd3f"
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a80ee92a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 601
        },
        "id": "a80ee92a",
        "outputId": "5db246c1-b10d-4943-b340-0bdd2e259145"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2301252461.py:5: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()  # For mixed precision\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\", line 50, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\", line 2865, in __getitems__\n    return [{col: array[i] for col, array in batch.items()} for i in range(n_examples)]\n                  ~~~~~^^^\nIndexError: index 3 is out of bounds for dimension 0 with size 3\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2301252461.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1514\u001b[0m                 \u001b[0mworker_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1515\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1516\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1518\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data, worker_idx)\u001b[0m\n\u001b[1;32m   1549\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1551\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;31m# be constructed, don't try to instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\", line 50, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\", line 2865, in __getitems__\n    return [{col: array[i] for col, array in batch.items()} for i in range(n_examples)]\n                  ~~~~~^^^\nIndexError: index 3 is out of bounds for dimension 0 with size 3\n"
          ]
        }
      ],
      "source": [
        "if len(train_dataset) < batch_size:\n",
        "    batch_size = len(train_dataset)\n",
        "\n",
        "epochs = 3\n",
        "scaler = GradScaler()  # For mixed precision\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
        "        labels = batch[\"labels\"].to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast():  # Mixed precision forward + loss\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "        scaler.scale(loss).backward()       # Scaled backward\n",
        "        scaler.step(optimizer)              # Step optimizer\n",
        "        scaler.update()                     # Update scale\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} Train Loss: {total_loss/len(train_loader):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f853d43",
      "metadata": {
        "id": "2f853d43"
      },
      "source": [
        "code graveyard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63b6f92c",
      "metadata": {
        "id": "63b6f92c"
      },
      "outputs": [],
      "source": [
        "# =======================\n",
        "# CONFIG\n",
        "# =======================\n",
        "label_cols = [\"is_ad\", \"is_relevant\", \"is_rant\", \"is_legit\"]\n",
        "meta_cols = [\"url_count\", \"phone_count\", \"caps_ratio\", \"rating\", \"user_review_count\"]  # adjust based on available\n",
        "max_len = 128\n",
        "batch_size = 16\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88ba10a1",
      "metadata": {
        "id": "88ba10a1"
      },
      "outputs": [],
      "source": [
        "df = cleaned_df\n",
        "df[label_cols] = df[label_cols].astype(float)\n",
        "\n",
        "X = df.drop(columns=label_cols)\n",
        "y = df[label_cols].values\n",
        "\n",
        "mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "train_idx, test_idx = next(mskf.split(X, y))\n",
        "\n",
        "train_df = df.iloc[train_idx]\n",
        "test_df = df.iloc[test_idx]\n",
        "\n",
        "# Then split train into train/val\n",
        "train_idx2, val_idx = next(MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "                           .split(train_df.drop(columns=label_cols), train_df[label_cols].values))\n",
        "val_df = train_df.iloc[val_idx]\n",
        "train_df = train_df.iloc[train_idx2]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cc08865",
      "metadata": {
        "id": "5cc08865"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def tokenize_batch(df):\n",
        "    encodings = tokenizer(\n",
        "        df[\"clean_text\"].tolist(),\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=max_len,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    meta = torch.tensor(df[meta_cols].fillna(0).values, dtype=torch.float32)\n",
        "    labels = torch.tensor(df[label_cols].values, dtype=torch.float32)\n",
        "    return encodings, meta, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fd4a767",
      "metadata": {
        "id": "0fd4a767"
      },
      "outputs": [],
      "source": [
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, encodings, meta, labels):\n",
        "        self.encodings = encodings\n",
        "        self.meta = meta\n",
        "        self.labels = labels\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.encodings[\"input_ids\"][idx],\n",
        "            \"attention_mask\": self.encodings[\"attention_mask\"][idx],\n",
        "            \"meta\": self.meta[idx],\n",
        "            \"labels\": self.labels[idx]\n",
        "        }\n",
        "\n",
        "train_enc, train_meta, train_labels = tokenize_batch(train_df)\n",
        "val_enc, val_meta, val_labels = tokenize_batch(val_df)\n",
        "test_enc, test_meta, test_labels = tokenize_batch(test_df)\n",
        "\n",
        "train_dataset = ReviewDataset(train_enc, train_meta, train_labels)\n",
        "val_dataset = ReviewDataset(val_enc, val_meta, val_labels)\n",
        "test_dataset = ReviewDataset(test_enc, test_meta, test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d5a8971",
      "metadata": {
        "id": "5d5a8971"
      },
      "outputs": [],
      "source": [
        "class ReviewGuardModel(nn.Module):\n",
        "    def __init__(self, backbone=\"prajjwal1/bert-tiny\", meta_dim=len(meta_cols)):\n",
        "        super().__init__()\n",
        "        self.enc = AutoModel.from_pretrained(backbone)\n",
        "        hid = self.enc.config.hidden_size\n",
        "        self.meta_net = nn.Sequential(nn.Linear(meta_dim, 64), nn.ReLU(), nn.Dropout(0.1), nn.Linear(64, 64), nn.ReLU())\n",
        "        self.fuse = nn.Linear(hid + 64, hid)\n",
        "        self.cls = nn.Linear(hid, len(label_cols))\n",
        "    def forward(self, input_ids, attention_mask, meta, labels=None):\n",
        "        x = self.enc(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:,0]\n",
        "        m = self.meta_net(meta)\n",
        "        z = torch.relu(self.fuse(torch.cat([x, m], dim=1)))\n",
        "        logits = self.cls(z)\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_f = nn.BCEWithLogitsLoss()\n",
        "            loss = loss_f(logits, labels)\n",
        "        return {\"loss\": loss, \"logits\": logits}\n",
        "\n",
        "model = ReviewGuardModel().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a75ccd1",
      "metadata": {
        "id": "9a75ccd1"
      },
      "outputs": [],
      "source": [
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        meta = batch[\"meta\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, meta=meta, labels=labels)\n",
        "        loss = outputs[\"loss\"]\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1} Train Loss: {total_loss/len(train_loader):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d907928d",
      "metadata": {
        "id": "d907928d"
      },
      "outputs": [],
      "source": [
        "pos_weights = torch.tensor([29.5641, 0.0374, 12.0096, 0.1678], dtype=torch.float32).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "686efebc",
      "metadata": {
        "id": "686efebc"
      },
      "outputs": [],
      "source": [
        "model_name = \"prajjwal1/bert-tiny\"  # tiny BERT for CPU\n",
        "max_len = 64\n",
        "batch_size = 2\n",
        "accum_steps = 4  # gradient accumulation\n",
        "epochs = 3\n",
        "meta_cols = [\"url_count\",\"phone_count\",\"caps_ratio\",\"user_review_count\"]\n",
        "label_cols = [\"is_ad\",\"is_relevant\",\"is_rant\",\"is_legit\"]\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02da97f4",
      "metadata": {
        "id": "02da97f4"
      },
      "outputs": [],
      "source": [
        "class ReviewDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, tokenizer, max_len=128):\n",
        "        self.texts = df[\"clean_text\"].tolist()\n",
        "        self.meta = df[meta_cols].fillna(0).to_numpy(dtype=np.float32)\n",
        "        self.labels = df[label_cols].to_numpy(dtype=np.float32)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
        "            \"meta\": torch.tensor(self.meta[idx], dtype=torch.float),\n",
        "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "        }\n",
        "\n",
        "class ReviewGuardModel(nn.Module):\n",
        "    def __init__(self, backbone=model_name, meta_dim=len(meta_cols)):\n",
        "        super().__init__()\n",
        "        self.enc = AutoModel.from_pretrained(backbone)\n",
        "        hid = self.enc.config.hidden_size\n",
        "        self.meta_net = nn.Sequential(\n",
        "            nn.Linear(meta_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.fuse = nn.Linear(hid + 64, hid)\n",
        "        self.cls = nn.Linear(hid, len(label_cols))\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, meta, labels=None):\n",
        "        x = self.enc(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0]\n",
        "        m = self.meta_net(meta)\n",
        "        z = torch.relu(self.fuse(torch.cat([x, m], dim=1)))\n",
        "        logits = self.cls(z)\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_f = nn.BCEWithLogitsLoss()\n",
        "            loss = loss_f(logits, labels)\n",
        "        return {\"loss\": loss, \"logits\": logits}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e14719b",
      "metadata": {
        "id": "7e14719b"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\", use_fast=True)\n",
        "train_dataset = ReviewDataset(train_df, tokenizer, max_len=128)\n",
        "test_dataset = ReviewDataset(test_df, tokenizer, max_len=128)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "888e5409",
      "metadata": {
        "id": "888e5409"
      },
      "outputs": [],
      "source": [
        "model = ReviewGuardModel().to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf0a4dba",
      "metadata": {
        "id": "bf0a4dba"
      },
      "outputs": [],
      "source": [
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        meta = batch[\"meta\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, meta=meta)\n",
        "        logits = outputs[\"logits\"]\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1} Train Loss: {total_loss/len(train_loader):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fae19836",
      "metadata": {
        "id": "fae19836"
      },
      "outputs": [],
      "source": [
        "def tune_thresholds(model, val_loader):\n",
        "    model.eval()\n",
        "    all_logits, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            meta = batch[\"meta\"].to(device)\n",
        "            labels = batch[\"labels\"].cpu().numpy()\n",
        "\n",
        "            logits = model(input_ids=input_ids, attention_mask=attention_mask, meta=meta)[\"logits\"].cpu().numpy()\n",
        "            all_logits.append(logits)\n",
        "            all_labels.append(labels)\n",
        "\n",
        "    all_logits = np.vstack(all_logits)\n",
        "    all_labels = np.vstack(all_labels)\n",
        "\n",
        "    thresholds = []\n",
        "    for i in range(all_labels.shape[1]):\n",
        "        best_thresh = 0.5\n",
        "        best_f1 = 0.0\n",
        "        for t in np.arange(0.1, 0.9, 0.05):\n",
        "            preds = (1 / (1 + np.exp(-all_logits[:, i])) >= t).astype(int)\n",
        "            f1 = f1_score(all_labels[:, i], preds)\n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_thresh = t\n",
        "        thresholds.append(best_thresh)\n",
        "    return thresholds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f366d70",
      "metadata": {
        "id": "5f366d70"
      },
      "outputs": [],
      "source": [
        "val_dataset = ReviewDataset(val_df)  # val_df is your validation DataFrame\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "thresholds = tune_thresholds(model, val_loader)\n",
        "\n",
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        meta = batch[\"meta\"].to(device)\n",
        "        labels = batch[\"labels\"].cpu().numpy()\n",
        "\n",
        "        logits = model(input_ids=input_ids, attention_mask=attention_mask, meta=meta)[\"logits\"].cpu().numpy()\n",
        "        preds = np.array([(1 / (1 + np.exp(-logits[:, i])) >= thresholds[i]).astype(int)\n",
        "                          for i in range(logits.shape[1])]).T\n",
        "        all_preds.append(preds)\n",
        "        all_labels.append(labels)\n",
        "\n",
        "all_preds = np.vstack(all_preds)\n",
        "all_labels = np.vstack(all_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1be124f1",
      "metadata": {
        "id": "1be124f1"
      },
      "outputs": [],
      "source": [
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for i, batch in enumerate(train_loader):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        meta = batch[\"meta\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, meta=meta, labels=labels)\n",
        "        loss = outputs[\"loss\"] / accum_steps\n",
        "        loss.backward()\n",
        "\n",
        "        if (i + 1) % accum_steps == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item() * accum_steps\n",
        "\n",
        "    print(f\"Epoch {epoch+1} Train Loss: {total_loss/len(train_loader):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72384b07",
      "metadata": {
        "id": "72384b07"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        meta = batch[\"meta\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        logits = model(input_ids=input_ids, attention_mask=attention_mask, meta=meta)[\"logits\"]\n",
        "        probs = torch.sigmoid(logits).cpu().numpy()\n",
        "        all_preds.append(probs)\n",
        "        all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "all_preds = np.vstack(all_preds)\n",
        "all_labels = np.vstack(all_labels)\n",
        "\n",
        "# Metrics per label\n",
        "metrics = {}\n",
        "for i, name in enumerate(label_cols):\n",
        "    ap = average_precision_score(all_labels[:, i], all_preds[:, i])\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(all_labels[:, i], (all_preds[:, i]>=0.5).astype(int), zero_division=0)\n",
        "    metrics[f\"{name}_ap\"] = ap\n",
        "    metrics[f\"{name}_prec\"] = prec[0]\n",
        "    metrics[f\"{name}_rec\"] = rec[0]\n",
        "    metrics[f\"{name}_f1\"] = f1[0]\n",
        "\n",
        "# Micro and macro F1\n",
        "metrics[\"micro_f1\"] = precision_recall_fscore_support(all_labels, (all_preds>=0.5).astype(int), average=\"micro\", zero_division=0)[2]\n",
        "metrics[\"macro_f1\"] = precision_recall_fscore_support(all_labels, (all_preds>=0.5).astype(int), average=\"macro\", zero_division=0)[2]\n",
        "\n",
        "print(\"Evaluation metrics:\", metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a00be80d",
      "metadata": {
        "id": "a00be80d"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n",
        "max_len = 128\n",
        "\n",
        "class ReviewDataset(IterableDataset):\n",
        "    def __init__(self, df, tokenizer, max_len, meta_cols, label_cols):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.meta_cols = meta_cols\n",
        "        self.label_cols = label_cols\n",
        "\n",
        "    def __iter__(self):\n",
        "        for _, row in self.df.iterrows():\n",
        "            enc = self.tokenizer(\n",
        "                row[\"clean_text\"],\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                max_length=self.max_len,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "            meta = torch.tensor([row[c] for c in self.meta_cols], dtype=torch.float32)\n",
        "            labels = torch.tensor([row[c] for c in self.label_cols], dtype=torch.float32)\n",
        "\n",
        "            yield {\n",
        "                \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
        "                \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
        "                \"meta\": meta,\n",
        "                \"labels\": labels\n",
        "            }\n",
        "\n",
        "train_dataset = ReviewDataset(train_df, tokenizer, max_len, meta_cols, label_cols)\n",
        "test_dataset = ReviewDataset(test_df, tokenizer, max_len, meta_cols, label_cols)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "058ce206",
      "metadata": {
        "id": "058ce206"
      },
      "outputs": [],
      "source": [
        "class ReviewGuardModel(nn.Module):\n",
        "    def __init__(self, backbone=\"prajjwal1/bert-tiny\", meta_dim=6):\n",
        "        super().__init__()\n",
        "        self.enc = AutoModel.from_pretrained(backbone)\n",
        "        hid = self.enc.config.hidden_size\n",
        "        self.meta_net = nn.Sequential(nn.Linear(meta_dim, 32), nn.ReLU(), nn.Dropout(0.1), nn.Linear(32, 32), nn.ReLU())\n",
        "        self.fuse = nn.Linear(hid + 32, hid)\n",
        "        self.cls = nn.Linear(hid, len(label_cols))\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, meta, labels=None):\n",
        "        x = self.enc(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:,0]\n",
        "        m = self.meta_net(meta)\n",
        "        z = torch.relu(self.fuse(torch.cat([x, m], dim=1)))\n",
        "        logits = self.cls(z)\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = nn.BCEWithLogitsLoss()(logits, labels)\n",
        "        return {\"loss\": loss, \"logits\": logits}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ReviewGuardModel(meta_dim=len(meta_cols)).to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b760fb12",
      "metadata": {
        "id": "b760fb12"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Optional: freeze transformer backbone for first few epochs\n",
        "freeze_backbone = True\n",
        "if freeze_backbone:\n",
        "    for param in model.enc.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)\n",
        "\n",
        "epochs = 3\n",
        "accum_steps = 4  # simulate larger batch by accumulating gradients\n",
        "scaler = GradScaler()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    batch_count = 0\n",
        "\n",
        "    for i, batch in enumerate(train_loader):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        meta = batch[\"meta\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        with autocast():  # mixed precision\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, meta=meta, labels=labels)\n",
        "            loss = outputs[\"loss\"] / accum_steps\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if (i + 1) % accum_steps == 0:\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item() * accum_steps\n",
        "        batch_count += 1\n",
        "\n",
        "    print(f\"Epoch {epoch+1} Train Loss: {total_loss / batch_count:.4f}\")\n",
        "\n",
        "# Optional: unfreeze backbone for fine-tuning\n",
        "if freeze_backbone:\n",
        "    for param in model.enc.parameters():\n",
        "        param.requires_grad = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "665d1236",
      "metadata": {
        "id": "665d1236"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        meta = batch[\"meta\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        logits = model(input_ids=input_ids, attention_mask=attention_mask, meta=meta)[\"logits\"]\n",
        "        probs = torch.sigmoid(logits).cpu().numpy()\n",
        "        all_preds.append(probs)\n",
        "        all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "import numpy as np\n",
        "all_preds = np.vstack(all_preds)\n",
        "all_labels = np.vstack(all_labels)\n",
        "preds_bin = (all_preds >= 0.5).astype(int)\n",
        "\n",
        "for i, label in enumerate(label_cols):\n",
        "    f1 = f1_score(all_labels[:,i], preds_bin[:,i])\n",
        "    prec = precision_score(all_labels[:,i], preds_bin[:,i])\n",
        "    rec = recall_score(all_labels[:,i], preds_bin[:,i])\n",
        "    print(f\"{label}: F1={f1:.3f}, Precision={prec:.3f}, Recall={rec:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Total memory: {torch.cuda.get_device_properties(0).total_memory/1024**3:.2f} GB\")\n",
        "    print(f\"Memory allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
        "    print(f\"Memory reserved: {torch.cuda.memory_reserved()/1024**3:.2f} GB\")\n",
        "\n",
        "print(f\"RAM available: {psutil.virtual_memory().total/1024**3:.2f} GB\")\n",
        "print(f\"RAM used: {psutil.virtual_memory().used/1024**3:.2f} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3iminld43z4",
        "outputId": "3b897983-703a-41c7-d687-f321156ac7e2"
      },
      "id": "l3iminld43z4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU available: True\n",
            "GPU: Tesla T4\n",
            "Total memory: 14.74 GB\n",
            "Memory allocated: 12.08 GB\n",
            "Memory reserved: 12.10 GB\n",
            "RAM available: 12.67 GB\n",
            "RAM used: 9.86 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify your data\n",
        "print(\"Training data info:\")\n",
        "print(f\"Shape: {train_df.shape}\")\n",
        "print(\"Columns:\", train_df.columns.tolist())\n",
        "print(\"\\nLabel distribution:\")\n",
        "print(train_df['is_relevant'].value_counts())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing values in training data:\")\n",
        "print(train_df.isnull().sum())\n",
        "\n",
        "# Check text length distribution\n",
        "train_df['text_length'] = train_df['review_text'].apply(len)\n",
        "print(f\"\\nText length stats:\\n{train_df['text_length'].describe()}\")\n",
        "\n",
        "# Sample a few examples\n",
        "print(\"\\nSample reviews:\")\n",
        "for i, row in train_df.head(3).iterrows():\n",
        "    print(f\"Review {i+1}: {row['review_text'][:100]}...\")\n",
        "    print(f\"Relevant: {row['is_relevant']}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBBSPSp26LJA",
        "outputId": "d08f1999-b229-41e9-9e7a-18fe27467c28"
      },
      "id": "jBBSPSp26LJA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data info:\n",
            "Shape: (8343, 19)\n",
            "Columns: ['review_text', 'rating', 'has_photo', 'author_name', 'user_review_count', 'business_name', 'category', 'source', 'review_id', 'comprehensive_review', 'is_ad', 'is_relevant', 'is_rant', 'is_legit', 'clean_text', 'url_count', 'phone_count', 'caps_ratio', 'text_length']\n",
            "\n",
            "Label distribution:\n",
            "is_relevant\n",
            "True     7615\n",
            "False     728\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Missing values in training data:\n",
            "review_text               0\n",
            "rating                  162\n",
            "has_photo                 0\n",
            "author_name               0\n",
            "user_review_count       162\n",
            "business_name             0\n",
            "category                  0\n",
            "source                    0\n",
            "review_id                 0\n",
            "comprehensive_review      0\n",
            "is_ad                     0\n",
            "is_relevant               0\n",
            "is_rant                   0\n",
            "is_legit                  0\n",
            "clean_text                0\n",
            "url_count                 0\n",
            "phone_count               0\n",
            "caps_ratio                0\n",
            "text_length               0\n",
            "dtype: int64\n",
            "\n",
            "Text length stats:\n",
            "count    8343.000000\n",
            "mean      148.843342\n",
            "std       215.590725\n",
            "min         1.000000\n",
            "25%        33.000000\n",
            "50%        75.000000\n",
            "75%       179.000000\n",
            "max      4045.000000\n",
            "Name: text_length, dtype: float64\n",
            "\n",
            "Sample reviews:\n",
            "Review 1: Love the convenience of this neighborhood carwash....\n",
            "Relevant: True\n",
            "--------------------------------------------------\n",
            "Review 2: 2 bathrooms (for a large 2 story building), 1 for each gender.  Door in men's room broken, and sleez...\n",
            "Relevant: True\n",
            "--------------------------------------------------\n",
            "Review 3: My favorite pizza shop hands down!...\n",
            "Relevant: True\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from google.colab import userdata\n",
        "    token = userdata.get('HF_TOKEN')\n",
        "    print(\"Token loaded from Colab secrets\")\n",
        "except:\n",
        "    token = getpass('Enter your Hugging Face token: ')\n",
        "\n",
        "# Login to Hugging Face\n",
        "try:\n",
        "    login(token=token)\n",
        "    print(\"✓ Successfully logged in to Hugging Face Hub\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ Login failed: {e}\")\n",
        "\n",
        "# Set environment variable\n",
        "os.environ['HF_TOKEN'] = token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBA22uftBaPv",
        "outputId": "595cadab-3562-4c0b-f649-f5e6abf166f4"
      },
      "id": "fBA22uftBaPv",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Hugging Face token: ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
            "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Successfully logged in to Hugging Face Hub\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Verify Model Loading\n",
        "MODEL_NAME = \"Qwen/Qwen3-8B\"\n",
        "\n",
        "if 'model' not in locals():\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "if 'tokenizer' not in locals():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"✓ Model and tokenizer ready!\")\n",
        "print(f\"Model device: {model.device}\")\n",
        "print(f\"Model dtype: {model.dtype}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97jPO5sN-xN9",
        "outputId": "8e9eee7d-c363-448b-fe0d-58786b7c2f03"
      },
      "id": "97jPO5sN-xN9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Model and tokenizer ready!\n",
            "Model device: cuda:0\n",
            "Model dtype: torch.float16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 512  # Reduced sequence length\n",
        "BATCH_SIZE = 1    # Small batch size for 8B model\n",
        "GRADIENT_ACCUMULATION_STEPS = 8  # Increase to get effective batch size\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_EPOCHS = 2\n",
        "LORA_R = 16\n",
        "LORA_ALPHA = 32\n",
        "LORA_DROPOUT = 0.05\n",
        "\n",
        "print(\"Memory-optimized configuration:\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\n",
        "print(f\"Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7SCplDuHR-k",
        "outputId": "9f50c7eb-0229-4a86-c3ca-569e8d1b2ac6"
      },
      "id": "y7SCplDuHR-k",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory-optimized configuration:\n",
            "Batch size: 1\n",
            "Gradient accumulation: 8\n",
            "Effective batch size: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_lm_dataset(df, tokenizer, max_length=MAX_LENGTH):\n",
        "    \"\"\"Prepare dataset for language model training\"\"\"\n",
        "    texts = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        text = f\"### Review Analysis Task:\\n\"\n",
        "        text += f\"Review Text: {row['review_text']}\\n\"\n",
        "\n",
        "        if 'rating' in row:\n",
        "            text += f\"Rating: {row['rating']}/5\\n\"\n",
        "\n",
        "        meta_cols = [\"url_count\", \"phone_count\", \"caps_ratio\", \"has_photo\", \"user_review_count\"]\n",
        "        for col in meta_cols:\n",
        "            if col in row and pd.notna(row[col]):\n",
        "                text += f\"{col.replace('_', ' ').title()}: {row[col]}\\n\"\n",
        "\n",
        "        text += \"### End of Review ###\\n\"\n",
        "        texts.append(text)\n",
        "\n",
        "    tokenized = tokenizer(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        padding=False,\n",
        "        max_length=max_length,\n",
        "        return_offsets_mapping=False\n",
        "    )\n",
        "\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "\n",
        "    return tokenized\n",
        "\n",
        "print(\"Preparing training data...\")\n",
        "train_tokenized = prepare_lm_dataset(train_df, tokenizer)\n",
        "val_tokenized = prepare_lm_dataset(val_df, tokenizer)\n",
        "\n",
        "train_hf_dataset = Dataset.from_dict(train_tokenized)\n",
        "val_hf_dataset = Dataset.from_dict(val_tokenized)\n",
        "\n",
        "print(f\"Training samples: {len(train_hf_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_hf_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8252s_LGbUr",
        "outputId": "af37df2e-8907-41d7-c51e-588755e4df0f"
      },
      "id": "-8252s_LGbUr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing training data...\n",
            "Training samples: 8343\n",
            "Validation samples: 1907\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "    ],\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Apply LoRA to model\n",
        "if isinstance(model, PeftModel) or hasattr(model, \"peft_config\"):\n",
        "    raise RuntimeError(\"Model already has PEFT adapters attached in this session. Restart runtime before proceeding.\")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Percentage trainable: {100 * trainable_params / total_params:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "ZzeJDAS_Hujn",
        "outputId": "f5491ab7-4958-4f75-aab7-8e549f6f30f5"
      },
      "id": "ZzeJDAS_Hujn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Model already has PEFT adapters attached in this session. Restart runtime before proceeding.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-349162442.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Apply LoRA to model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPeftModel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"peft_config\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model already has PEFT adapters attached in this session. Restart runtime before proceeding.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_peft_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlora_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Model already has PEFT adapters attached in this session. Restart runtime before proceeding."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # Causal language modeling\n",
        ")\n",
        "\n",
        "# Training arguments optimized for T4\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./qwen3-finetuned\",\n",
        "    overwrite_output_dir=True,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    save_steps=100,\n",
        "    logging_steps=10,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.1,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    fp16=True,  # Mixed precision training\n",
        "    report_to=\"none\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    gradient_checkpointing=True,  # Memory optimization\n",
        "    optim=\"adamw_torch\",\n",
        "    max_grad_norm=1.0,\n",
        ")\n",
        "training_args.parallelism_config = None\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_hf_dataset,\n",
        "    eval_dataset=val_hf_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"Trainer configured successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "cXO7Io8rHzTQ",
        "outputId": "b4a95715-6c81-49da-e995-352ac96e44f9"
      },
      "id": "cXO7Io8rHzTQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2252732418.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Initialize Trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m trainer = Trainer(\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    618\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"quantization_method\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mQuantizationMethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBITS_AND_BYTES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m         ):\n\u001b[0;32m--> 620\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_move_model_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;31m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_move_model_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 913\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    914\u001b[0m         \u001b[0;31m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mParallelMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPU\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tie_weights\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1367\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1369\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1360\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Cannot copy out of meta tensor; no data!\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m                     raise NotImplementedError(\n\u001b[0m\u001b[1;32m   1363\u001b[0m                         \u001b[0;34mf\"{e} Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m                         \u001b[0;34mf\"when moving module from meta to a different device.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 256\n",
        "\n",
        "def prepare_lm_dataset(df, tokenizer, max_length=MAX_LENGTH):\n",
        "    texts = []\n",
        "    for _, row in df.iterrows():\n",
        "        text = f\"### Review Analysis Task:\\nReview Text: {row['review_text']}\\n\"\n",
        "        if 'rating' in row: text += f\"Rating: {row['rating']}/5\\n\"\n",
        "        if 'is_relevant' in row:\n",
        "            relevance = \"Relevant\" if row['is_relevant'] else \"Irrelevant\"\n",
        "            text += f\"Relevance: {relevance}\\n\"\n",
        "        text += \"### End of Review ###\\n\"\n",
        "        texts.append(text)\n",
        "\n",
        "    tokenized = tokenizer(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        padding=False,\n",
        "        max_length=max_length\n",
        "    )\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    return tokenized\n",
        "\n",
        "train_tokenized = prepare_lm_dataset(train_df, tokenizer)\n",
        "val_tokenized = prepare_lm_dataset(val_df, tokenizer)\n",
        "\n",
        "train_hf_dataset = Dataset.from_dict(train_tokenized)\n",
        "val_hf_dataset = Dataset.from_dict(val_tokenized)\n",
        "\n",
        "print(f\"Training samples: {len(train_hf_dataset)}, Validation samples: {len(val_hf_dataset)}\")"
      ],
      "metadata": {
        "id": "sSV0JWVGJzx-"
      },
      "id": "sSV0JWVGJzx-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"clf\", MultiOutputClassifier(LogisticRegression(max_iter=200)))\n",
        "])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "print(\"Validation score:\", pipeline.score(X_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RbyRL17N95s",
        "outputId": "7a63c6fe-8c06-4e13-b6f8-8fd9d2bdb8ef"
      },
      "id": "0RbyRL17N95s",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 0.8384897745149449\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"Qwen/Qwen2-0.5B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "def get_embeddings(texts, tokenizer, model, max_length=128, batch_size=16):\n",
        "    all_embeddings = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i+batch_size]\n",
        "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
        "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            batch_emb = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "        all_embeddings.append(batch_emb)\n",
        "    return np.vstack(all_embeddings)\n",
        "\n",
        "# Get Qwen embeddings for clean_text\n",
        "train_text_emb = get_embeddings(train_df[\"clean_text\"].tolist(), tokenizer, model)\n",
        "val_text_emb = get_embeddings(val_df[\"clean_text\"].tolist(), tokenizer, model)\n",
        "test_text_emb = get_embeddings(test_df[\"clean_text\"].tolist(), tokenizer, model)\n",
        "\n",
        "# Meta features\n",
        "meta_cols = [\"url_count\",\"phone_count\",\"caps_ratio\",\"rating\",\"has_photo\",\"user_review_count\"]\n",
        "train_meta = train_df[meta_cols].values\n",
        "val_meta = val_df[meta_cols].values\n",
        "test_meta = test_df[meta_cols].values\n",
        "\n",
        "# Combine embeddings + meta\n",
        "X_train = np.hstack([train_text_emb, train_meta])\n",
        "X_val = np.hstack([val_text_emb, val_meta])\n",
        "X_test = np.hstack([test_text_emb, test_meta])\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Multi-label classifier\n",
        "y_train = train_df[[\"is_ad\",\"is_relevant\",\"is_rant\"]].values\n",
        "y_val = val_df[[\"is_ad\",\"is_relevant\",\"is_rant\"]].values\n",
        "\n",
        "clf = MultiOutputClassifier(LogisticRegression(max_iter=200))\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Validation score:\", clf.score(X_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "VnInjfecNAx3",
        "outputId": "f5a3fc36-b1da-4636-f9fd-3a282158e346"
      },
      "id": "VnInjfecNAx3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-433953297.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiOutputClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation score:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/multioutput.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfitted\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \"\"\"\n\u001b[0;32m--> 543\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mestimator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/multioutput.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0mrouted_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sample_weight\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n\u001b[0m\u001b[1;32m    275\u001b[0m             delayed(_fit_estimator)(\n\u001b[1;32m    276\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mrouted_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1984\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1985\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1986\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1988\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1912\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1914\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1915\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/multioutput.py\u001b[0m in \u001b[0;36m_fit_estimator\u001b[0;34m(estimator, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1220\u001b[0m             \u001b[0m_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m         X, y = validate_data(\n\u001b[0m\u001b[1;32m   1223\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2959\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2961\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2962\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0mensure_all_finite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deprecate_force_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_all_finite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1371\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m   1108\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     _assert_all_finite_element_wise(\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             )\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for obj in [\"model\", \"tokenizer\"]:\n",
        "    if obj in globals():\n",
        "        del globals()[obj]\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqULQ07jJcMb",
        "outputId": "adc2982a-7b71-4f6c-bd13-4570d63d89a5"
      },
      "id": "PqULQ07jJcMb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"Qwen/Qwen3-8B\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True\n",
        ").to(\"cuda\")\n",
        "\n",
        "print(\"Base model loaded:\", model.device, model.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195,
          "referenced_widgets": [
            "493d50bf8e994574b60e44f5a4f1aaf6",
            "bde67800534a4b4fa539f03e4e53a00e",
            "406c5a535437458bbeacd4ac9dccbc4f",
            "72d61cf1059c465f97be6ddbb4608f45",
            "27a762bd470445b084fb6590c2f37636",
            "9532cc8bea0b4edb9f6dfe5ad5e51705",
            "2af69a7b98a2462a99bd589dea41fad8",
            "0c9c9079255540ffa1aa0763993d5784",
            "36e8809b290946d9aadad85b098e39a3",
            "945fd2547a114ae59364575dffa20a58",
            "e712eba866ae4d149876ca4bc9caeece"
          ]
        },
        "id": "RE-4ywTJJlY1",
        "outputId": "19e5c8cb-a2bf-4818-c589-947c6c4a2bb9"
      },
      "id": "RE-4ywTJJlY1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "493d50bf8e994574b60e44f5a4f1aaf6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LORA_R = 16\n",
        "LORA_ALPHA = 32\n",
        "LORA_DROPOUT = 0.05\n",
        "\n",
        "if isinstance(model, PeftModel) or hasattr(model, \"peft_config\"):\n",
        "    raise RuntimeError(\"Model already has PEFT adapters. Restart runtime if needed.\")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Trainable params: {trainable_params:,} / {total_params:,} ({100 * trainable_params / total_params:.2f}%)\")"
      ],
      "metadata": {
        "id": "VFchSEgKJuq4"
      },
      "id": "VFchSEgKJuq4",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "bfb37d5d",
        "03ff6b75",
        "2ydi6f0q41QW"
      ],
      "gpuType": "T4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "493d50bf8e994574b60e44f5a4f1aaf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bde67800534a4b4fa539f03e4e53a00e",
              "IPY_MODEL_406c5a535437458bbeacd4ac9dccbc4f",
              "IPY_MODEL_72d61cf1059c465f97be6ddbb4608f45"
            ],
            "layout": "IPY_MODEL_27a762bd470445b084fb6590c2f37636"
          }
        },
        "bde67800534a4b4fa539f03e4e53a00e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9532cc8bea0b4edb9f6dfe5ad5e51705",
            "placeholder": "​",
            "style": "IPY_MODEL_2af69a7b98a2462a99bd589dea41fad8",
            "value": "Loading checkpoint shards:  40%"
          }
        },
        "406c5a535437458bbeacd4ac9dccbc4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c9c9079255540ffa1aa0763993d5784",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_36e8809b290946d9aadad85b098e39a3",
            "value": 2
          }
        },
        "72d61cf1059c465f97be6ddbb4608f45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_945fd2547a114ae59364575dffa20a58",
            "placeholder": "​",
            "style": "IPY_MODEL_e712eba866ae4d149876ca4bc9caeece",
            "value": " 2/5 [00:47&lt;01:12, 24.17s/it]"
          }
        },
        "27a762bd470445b084fb6590c2f37636": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9532cc8bea0b4edb9f6dfe5ad5e51705": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2af69a7b98a2462a99bd589dea41fad8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c9c9079255540ffa1aa0763993d5784": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36e8809b290946d9aadad85b098e39a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "945fd2547a114ae59364575dffa20a58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e712eba866ae4d149876ca4bc9caeece": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9bed4ee155924991ac3a8e0b171534f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a03a2e58cb704e24a8b7bb6f563a3e57",
              "IPY_MODEL_9e6028ad6df34afbb334d1854317fa15",
              "IPY_MODEL_c177bed50b44453ebf3075c0c8dca3e5"
            ],
            "layout": "IPY_MODEL_c93d71abbbe14b73829dab5ef47fec32"
          }
        },
        "a03a2e58cb704e24a8b7bb6f563a3e57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31f677824421490187a91716b7b13be1",
            "placeholder": "​",
            "style": "IPY_MODEL_65a914f4a864483d9789f835716bd3d4",
            "value": "Map: 100%"
          }
        },
        "9e6028ad6df34afbb334d1854317fa15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47f0bd93dde44f7999a4a169cd817668",
            "max": 8180,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2ad87e6e2af840c7afc6a79fb019effd",
            "value": 8180
          }
        },
        "c177bed50b44453ebf3075c0c8dca3e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89a6f136e1dc40eb9991d98eabae9bdd",
            "placeholder": "​",
            "style": "IPY_MODEL_0743142c7e8f4bce8f8d811c2adc9566",
            "value": " 8180/8180 [00:05&lt;00:00, 1515.04 examples/s]"
          }
        },
        "c93d71abbbe14b73829dab5ef47fec32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31f677824421490187a91716b7b13be1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65a914f4a864483d9789f835716bd3d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "47f0bd93dde44f7999a4a169cd817668": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ad87e6e2af840c7afc6a79fb019effd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "89a6f136e1dc40eb9991d98eabae9bdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0743142c7e8f4bce8f8d811c2adc9566": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "506a3e7320bd4da7a1d6dab730ef2f80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b8d72c1dd5704d6eb345aa6738db184f",
              "IPY_MODEL_9022492a03d84fb5ae3ae8de6a9c9bc5",
              "IPY_MODEL_0d548295a9ed466bb73550974d02eca2"
            ],
            "layout": "IPY_MODEL_8283989e3321459eac606fe4b09dda10"
          }
        },
        "b8d72c1dd5704d6eb345aa6738db184f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0382a99d6fa7410980c568979c3b0a71",
            "placeholder": "​",
            "style": "IPY_MODEL_75602bf9395046edb92ed6a80ceaf29a",
            "value": "Map: 100%"
          }
        },
        "9022492a03d84fb5ae3ae8de6a9c9bc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_155b1addd236490193c475e900613594",
            "max": 1866,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_12e41a4a51794203b44c39a45dcce171",
            "value": 1866
          }
        },
        "0d548295a9ed466bb73550974d02eca2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37cf38389093453bb642d582138c20e0",
            "placeholder": "​",
            "style": "IPY_MODEL_95f5a95ef2a34aea8a31d186731884a6",
            "value": " 1866/1866 [00:01&lt;00:00, 1766.72 examples/s]"
          }
        },
        "8283989e3321459eac606fe4b09dda10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0382a99d6fa7410980c568979c3b0a71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75602bf9395046edb92ed6a80ceaf29a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "155b1addd236490193c475e900613594": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12e41a4a51794203b44c39a45dcce171": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "37cf38389093453bb642d582138c20e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95f5a95ef2a34aea8a31d186731884a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}